# -*- coding: utf-8 -*-
"""SpeakerRecognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dyo27lUTOWQRi8zQL2zJgQKBE92fs87B
"""

!pip install librosa soundfile --quiet

import os
import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import IPython.display as ipd

from google.colab import drive
drive.mount('/content/drive')

DATASET_PATH = "/content/drive/MyDrive/Colab Notebooks/16000_pcm_speeches"

SPEAKERS = ['Benjamin_Netanyau', 'Jens_Stoltenberg', 'Julia_Gillard', 'Magaret_Tarcher', 'Nelson_Mandela']

# Özellik çıkarımı Mel Spectrogram
def extract_features(file_path, max_pad_len=128):
    try:
        audio, sr = librosa.load(file_path, sr=16000)
        melspec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128)
        log_melspec = librosa.power_to_db(melspec)

        if log_melspec.shape[1] < max_pad_len:
            pad_width = max_pad_len - log_melspec.shape[1]
            log_melspec = np.pad(log_melspec, pad_width=((0, 0), (0, pad_width)), mode='constant')
        else:
            log_melspec = log_melspec[:, :max_pad_len]

        return log_melspec
    except Exception as e:
        print("Error encountered while parsing file: ", file_path)
        return None

features = []
labels = []

for speaker in SPEAKERS:
    speaker_folder = os.path.join(DATASET_PATH, speaker)
    for file_name in os.listdir(speaker_folder):
        if file_name.endswith('.wav'):
            file_path = os.path.join(speaker_folder, file_name)
            data = extract_features(file_path)
            if data is not None:
                features.append(data)
                labels.append(speaker)

X = np.array(features)
X = X[..., np.newaxis]  # CNN için kanal eklenir
y = LabelEncoder().fit_transform(labels)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(len(SPEAKERS), activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Training vs Validation Accuracy')
plt.show()

# Loss
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Model Loss')
plt.grid(True)
plt.show()

test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc:.2f}")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

y_pred = model.predict(X_test)
y_pred_labels = np.argmax(y_pred, axis=1)

cm = confusion_matrix(y_test, y_pred_labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=SPEAKERS)
disp.plot(xticks_rotation=45, cmap='Blues')
plt.title('Confusion Matrix')
plt.show()

from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_labels, target_names=SPEAKERS))